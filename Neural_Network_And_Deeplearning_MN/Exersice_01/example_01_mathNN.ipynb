{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Basic Mathematics Behind the Neural Network\n",
    "---\n",
    "The **neural network (NN)** is a learner made of several **neuron** idealy which is similar to **boosting algorithem**, see [Machine Learning in Python/Chapter 7/Example 3](../../Machine_Learning_in_Python_SR/Chapter_07/example_03_adaBoost.ipynb), using many weaker learner to ensemble a strong learner. Each neuron or weaker learner is expected to be with responsibility a simple and particular model. The basic structure of the nueral network contains 3 layers : **input layer**, **hidden layers** and **output layer**, as in following figure: \n",
    "\n",
    "![../doc/nn.png](../doc/nn.png)\n",
    "(Credited from [Neural Networks and Deep Learning/Chapter 1.](http://neuralnetworksanddeeplearning.com/chap1.html))\n",
    "\n",
    "The input layer is the first layer of neural network to encode the coming data to the hidden layers, and the number of the neurons, $n_{i}$, usually is the same with the dimension of the features; the hidden layers can contain the several layers, $l_{h}$, and the different sizes of neurons; And the output layer connects with the last layer of the hidden layers to decode thier results to final prediction, and the number of the neuron, $n_o$, is usually as same as the target class, of course it can be reduced or transfer to different representation. It is sometimes simply called \"**$n_il_hn_o$-NN**\". The neuron is actually made by the ***Character function*** and ***Active function*** which hold the data in mathematical way. The *Character function* is usually defined as \n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}\\mathbf{X}+b\\ ,\n",
    "$$\n",
    "\n",
    "which is as the *perceptron*, detials see [Machine Learning in Python/Chapter 2](../../Machine_Learning_in_Python_SR/Chapter_02), where $\\mathbf{X}$ is the input data with the vector form which recores the features; and $\\mathbf{w}$ and $b$ are the weight and bias that are going to be learned, respectively.\n",
    "The *Active function* is the fuction transfering the value of character fucntion to the \"proper\" output, e.g. the ***linear function*** or ***sigmoid function*** as \n",
    "\n",
    "$$\n",
    "a(z) = z\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "a(z)=\\frac{1}{1+\\exp{(-z)}}\\ , \n",
    "$$\n",
    "\n",
    "respectively. Of course there are several variant active functions can be used, but they are dependent on the property of the popurse and the chosen cost function. However, the neural netwrok can be either ***\"broader\"*** or ***\"deeper\"***, i.e. the number of neurons or layers can can be as many as possible, respectively. From the results of the expriences, the deeper neural network has better performance than broader one. The deeper type of the neural network is also called **Deeplearning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the model learning (training), we start from the input layer. The neurons of the input layer will encode the input data and output the results by *character function* and *active function* to next hiden layer. Each neuron of the first hiden layer will receive the input from all the output of the input layer and return the new output for the next hiden layer again, i.e. the neuron caculates the output with the *character function* and *active function*. The next, even the next after the next, all the hiden layers will do the same procedure as the first layer. In the end, the output will pass to output layer to obtain the prediction which we are interested.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several alternative algorithms made with the different cost functions, active functions, regularization method, iteration method etc.. However, the main sprit of machine learning is not changed, which looks for the lower cost (error), i.e. the global minimum. But to caculate the cost and update the weights and biases for all neurons sounds crazy and impossible, since the dimensions of the variables are too huge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
