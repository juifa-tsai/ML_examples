{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Basic Mathematics Behind the Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "The **neural network (NN)** is a learner made of several **neuron** idealy which is similar to **boosting algorithem**, see [Machine Learning in Python/Chapter 7/Example 3](../../Machine_Learning_in_Python_SR/Chapter_07/example_03_adaBoost.ipynb), using many weaker learner to ensemble a strong learner. Each neuron is expected to have the responsibility for a simple and particular model. The output (prediction) of each neuron will be the input of the next neurons, and they are connected as like a network. The basic structure of the nueral network contains 3 layers : **input layer**, **hidden layers** and **output layer**, as in following figure: \n",
    "\n",
    "![nn](../doc/nn.png)\n",
    "(Credited from [\"Using neural nets to recognize handwritten digits\"](http://neuralnetworksanddeeplearning.com/chap1.html))\n",
    "\n",
    "The input layer is the first layer of neural network to encode the coming data to the hidden layers, and the number of the neurons, $n_{i}$, usually is the same with the dimension of the features; the hidden layers can contain the several layers, $l_{h}$, and the different sizes of neurons; And the output layer connects with the last layer of the hidden layers to decode thier results to final prediction, and the number of the neuron, $n_o$, is usually as same as the target class, of course it can be reduced or transfer to different representation. It is sometimes simply called \"**$n_il_hn_o$-NN**\". The neuron is actually made by the ***Character function*** and ***Active function*** which hold the data in mathematical way. The *Character function* is usually defined as \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "z = \\mathbf{w}\\cdot\\mathbf{X}+b\\ \n",
    "}\\ ,\n",
    "$$\n",
    "\n",
    "which is as the ***perceptron***, details see [Machine Learning in Python/Chapter 2](https://github.com/juifa-tsai/workbook_MachineLearning/blob/master/Machine_Learning_in_Python_SR/Chapter_02/README.md), where $\\mathbf{X}$ is the input data with the vector form which recores the features; and $\\mathbf{w}$ and $b$ are the weight vector correspoding to features and bias, respectively. Both variables are going to be learned during training. Then, the *Active function* is the fuction transfering the value of character fucntion to the \"proper\" output as\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "a=\\sigma(z)\\ \n",
    "}\\ .\n",
    "$$\n",
    "\n",
    "For instance, the common used function $\\sigma$ can be ***linear function*** or ***sigmoid function*** as \n",
    "\n",
    "$$\n",
    "\\sigma(z)=z\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+\\exp{(-z)}}\\ , \n",
    "$$\n",
    "\n",
    "respectively. Of course there are several variant active functions can be used, but they are dependent on the property of the popurse and the chosen cost function. However, the neural netwrok can be either ***\"broader\"*** or ***\"deeper\"***, i.e. the number of neurons or layers can can be as many as possible, respectively. From the results of the expriences, the deeper neural network has better performance than broader one. The deeper type of the neural network is also called **Deeplearning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variables definition\n",
    "\n",
    "During the model learning (training), we start from the input layer. The neurons of the input layer will encode the input data and output the results by *character function* and *active function* to next hidden layer. Each neuron of the first hidden layer will receive the input from all the output of the input layer and return the new output for the next hidden layer again, i.e. the neuron caculates the output with the *character function* and *active function*. The next, even the next after the next, all the hidden layers will do the same procedure as the first layer. In the end, the output of the last hidden layer will pass through the output layer and decode to the prediction which we are interested. Since the learning procedure is from the layer to next layer, it is also called ***feedforward neural network***.   \n",
    "\n",
    "However, you can imagine that there must be many weights and biases during learning (training). To avoid the confusion, we have to give them the proper notations. Since all the variables are connected between two layers, we denote its layer is $l$, its one of neuron is $i$, and the input from one of the neurons from previous layer, $l-1$, is denoted $j$. Thus, $w_{ji}^l$ is the weight from the $k^{\\text{th}}$ neuron in the $(l-1)^{\\text{th}}$ layer to the $j^{\\text{th}}$ neuron in the $l^{\\text{th}}$ layer. The example is illustrated in following figure\n",
    "\n",
    "![w](../doc/w.png) \n",
    "(Credited from [\"How the backpropagation algorithm works\"](http://neuralnetworksanddeeplearning.com/chap2.html))\n",
    "\n",
    "The bias and the value of *active function* are redifined as $b_{i}^l$ and $a_{i}^l$, the example is illustrated in following figure\n",
    "\n",
    "![b](../doc/ba.png)\n",
    "(Credited from [\"How the backpropagation algorithm works\"](http://neuralnetworksanddeeplearning.com/chap2.html))\n",
    "\n",
    "where $a_i^l$ is equal to $\\sigma(z_i^l)$ which the *character function* is redifined as\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "z_j^l = \\sum_{j}w_{ji}^la_i^{l-1}+b_j^l\\ }\\ ,\n",
    "$$\n",
    "\n",
    "where $a_i^{l-1}$ is the output of *avctive function* from previous layer $l-1$ as $\\sigma(z_i^{l-1})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training method\n",
    "#### 3.1. Variable updating & cost minimization\n",
    "There are several alternative algorithms made with the different cost functions, active functions, regularization method, iteration method etc.. However, the main sprit of machine learning is not changed, which looks for the lower cost (error), i.e. ***global minimum***. The training variables, the weights and biases, are supposed to be updated for reducing the cost in each epoch iteratively. The updates are defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\Delta w_{ji}^l = -\\eta\\frac{\\partial C}{\\partial w_{ji}^l} \\\\\n",
    "\\Delta b_{j}^l = -\\eta\\frac{\\partial C}{\\partial b_{j}^l}\\ ,\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "where $C$ is the total cost from output layer; $\\eta$ is the learning rate to control the speed of the variable's movement. Since the cost is expected to be decreasing when we iteratively change the variables, the gradient is negative, i.e. the changes of the cost is opposite with respect to the direction of the variables' moves. Thus, the updated variables can be written as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w_{ji}^l&\\to w_{ji}^l+\\Delta w_{ji}^l \\\\\n",
    "b_{j}^l&\\to b_{j}^l+\\Delta b_{j}^l\\ .\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But to caculate the updates of the weights, $\\Delta w_{jk}^l$, and biases, $\\Delta b_{k}^l$, for all neurons sounds crazy and impossible, since the dimensions of the variables are too huge, sometimes hundreds and thousands. \n",
    "\n",
    "#### 3.2. Backpropagetion\n",
    "Fortunately, there is a simple and efficeint way, called **backpropagation**, to obtain all the updates of variables for reducing the cost iteratively in each epoch. The major mathematics behind the backpropagation is just the **chain rule** of the **partial derivative** with respect to the variables. Here I list the major four equations from the backpropagation first, and the derivation will be shown later. The first two equations are about the gradient of the costs, i.e. the rate of error changes, with respect to the input of the layer, i.e. it is with respect to the variation of the *characher function*. Since the final cost in the output layer, $C$, is the only value can be obtained from data directly after feedforward training, the **backpropagation** method can be started from the output layer, when $l=L$, to the previous layer, previour two layer, and so on, until reaching the input layer. The two equations are presented and derivated as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\frac{\\partial C}{\\partial z^L}=\\delta^L=\\nabla_aC\\odot\\sigma'(z^{L}) \\\\\n",
    "&\\frac{\\partial C}{\\partial z^l}=\\delta^l=((\\mathbf{w}^{l+1})^\\text{T}\\delta^{l+1})\\odot\\sigma'(z^l)\\ ,\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the $\\delta$ is denoted as the gradient of cost in the form of vector for each layer, and the dimension is the number of the neurons. The other two equations are about the gradient of the costs with respect to the training variables, i.e. the weights and biases. Both are used for updating the variables in each training epoch. They can be obtained by the partial derivative and result to\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\frac{\\partial C}{\\partial b_j^l}=\\delta_j^l \\\\\n",
    "&\\frac{\\partial C}{\\partial w_{ji}^l}=a_i^{l-1}\\delta_j^l\\ .\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, accoding to the four equations of backpropagetion, we can clearly look into this *\"black box\"*, seeing how it works and learns, since they present the cost error in each layer with respect to the interested variable. The equations can be also rewritten and simplified with much meaningful representation as \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&\\delta^L&=\\nabla_aC\\odot\\sigma'(z^{L}) \\\\\n",
    "&\\delta^l&=((\\mathbf{w}^{l+1})^\\text{T}\\delta^{l+1})\\odot\\sigma'(z^l)\\\\\n",
    "&b_{j}^l&=-\\eta\\delta_j^l \\\\\n",
    "&w_{ji}^l&=-\\eta a_i^{l-1}\\delta_j^l\\ .\n",
    "\\end{split}\n",
    "\\end{equation}}\n",
    "$$\n",
    "\n",
    "##### 3.2.1. Derivation of  $\\delta^L$, the cost error for the output layer\n",
    "The cost error of the output layer is written with $\\frac{\\partial C}{\\partial z_j^L}$ and can be expended by the ***chain rule*** of the partial differential with respect to *active function*, $a_j^l=\\sigma(z_j^l)$, as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_j^L}=\\frac{\\partial C}{\\partial a_j^l}\\frac{\\partial a_j^l}{\\partial z_j^L}\\ ,\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial C}{\\partial a_j^l}$ is the gradient of the final cost with respect to *active function*, and it can be denoted as $\\nabla_{a}C$ in the form of vector for all neurons in output layer; $\\frac{\\partial a_j^l}{\\partial z_j^L}$ can be also rewritten as $\\frac{\\partial \\sigma(z^l)}{\\partial z^L}$ for all neurons in output layer. Thus, we can obtain the equation as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C}{\\partial z^L}&=\\nabla_aC\\odot\\sigma'(z^{L})\\\\ \n",
    "&=\\delta^L\\ , \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\sigma'(z^{L})=\\frac{\\partial \\sigma(z^l)}{\\partial z^L}$; $\\odot$ is ***Hadamard product***, the inner product only apply the element with the same neuron. \n",
    "\n",
    "##### 3.2.2. Derivation of  $\\delta^l$, the cost error for other layers \n",
    "The cost error of other layers is written with $\\frac{\\partial C}{\\partial z_j^l}$ and can be expended by the ***chain rule*** of the partial differential with respect to the *character function* of the next layer, $z_j^{l+1}$, since the cost error is expected to be propagated to next layer and correlated withn layers. It can be written as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z_j^l}=\\sum_k\\frac{\\partial C}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\ ,\n",
    "$$\n",
    "\n",
    "where $z_k^{l+1} = \\sum_j{w_{kj}^{l+1}a_j^l}+b_k$, and it can be \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} &= \\frac{\\partial z_k^{l+1}}{\\partial a_j^l}\\frac{\\partial a_j^l}{\\partial z_j^l} \\\\\n",
    "&=w_{kj}^{l+1}\\frac{\\partial \\sigma(z_j^l)}{\\partial z_j^l}\\\\\n",
    "&=w_{kj}^{l+1}\\sigma'(z_j^{L})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $a_j^l=\\sigma(z_j^l)$ and $\\sigma'(z_j^{L})=\\frac{\\partial \\sigma(z_j^l)}{\\partial z^L}$. By replacing the above equation, we can obtain \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C}{\\partial z_j^l}&=(\\sum_kw_{kj}^{l+1}\\frac{\\partial C}{\\partial z_k^{l+1}})\\sigma'(z_j^{L})\\\\\n",
    "&=((\\mathbf{w}_{j}^{l+1})^\\text{T}\\delta^{l+1})\\sigma'(z_j^{L})\\ ,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\delta^{l+1}$ is the cost error of the next layer. Thus, for all neurons, the equation can be summerized to \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C}{\\partial z^l}&=((\\mathbf{w}^{l+1})^\\text{T}\\delta^{l+1})\\odot\\sigma'(z^l)\\\\\n",
    "&=\\delta^l \\ ,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\odot$ is ***Hadamard product***, the inner product only apply the element with the same neuron.\n",
    "\n",
    "##### 3.2.3. Derivation of  $\\frac{\\partial C}{\\partial b_j^l}$, the gradient w.r.t. bias\n",
    "The cost gradient with respect to the bias can be simplified by the chain rule of the patial defferential as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C}{\\partial b_j^l}&=\\frac{\\partial z_j^l}{\\partial b_j^l}\\frac{\\partial C}{\\partial z_j^l}\\\\\n",
    "&=\\frac{\\partial C}{\\partial z_j^l}\\\\\n",
    "&=\\delta_{j}^{l}\\ ,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial z_j^l}{\\partial b_j^l}=1$, and the gradient is equivalent to the cost error of the neuron. Thus, the bias update becomes\n",
    "\n",
    "$$\n",
    "\\Delta b_{j}^l = -\\eta\\delta_{j}^{l}\n",
    "$$\n",
    "\n",
    "##### 3.2.4. Derivation of  $\\frac{\\partial C}{\\partial w_{ji}^l}$, the gradient w.r.t. weight\n",
    "The cost gradient with respect to the weight can be simplified by the chain rule of the patial defferential as\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial C}{\\partial w_{ji}^l}&=\\frac{\\partial z_j^l}{\\partial w_{ji}^l}\\frac{\\partial C}{\\partial z_j^l}\\\\\n",
    "&=a_{i}^{l-1}\\delta_{j}^{l}\\ ,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial z_j^l}{\\partial w_{ji}^l}=a_{i}^{l-1}$. The gradient shows the changes coming from the input, $a_{i}^{l-1}$, from previous layer and the output cost error, $\\delta_{j}^{l}$. Thus, the weight update becomes\n",
    "\n",
    "$$\n",
    "\\Delta w_{ji}^l = -\\eta a_{i}^{l-1}\\delta_{j}^{l}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Procedure with Backpropagation\n",
    "In the case of the *cost function*, $C$, and *active function*, $a=\\sigma(z)$, are defined with certain equations, then we can simplify the four equations to analytic form. To achive the backpropagation method to train the network from data, the algorithm is as following: \n",
    "\n",
    "1. Feedforward training the network from the input data.  \n",
    "2. Caculate the cost error $\\delta^{L}$ for the output layer,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\delta^L=\\nabla_aC\\odot\\sigma'(z^{L})\\ .\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "3. Obtain the bias updates $\\Delta b_j^L$ for the output layer,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\Delta b_{j}^{L}=-\\eta\\delta_j^L\\ .\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "4. Obtain the weights between $L^{\\text{th}}$ and $(L-1)^{\\text{th}}$ layer, \n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\Delta w_{ji}^{L}=-\\eta a_i^{L-1}\\delta_j^L\\ .\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "5. Use the reaults from the precedures **2.~4.** to obtain the cost error of $(L-1)^{\\text{th}}$ layer, \n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\delta^{L-1} = ((\\mathbf{w}^{L})^\\text{T}\\delta^{L})\\odot\\sigma'(z^{L-1})\\ .\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "6. Iterate to obtain the weights and biases for $(L-1)^{\\text{th}}$ layer as same as the precedures **3.** and **4.**, and then obtain the next cost error for $(L-2)^{\\text{th}}$ layer as the precedure **5.** until reaching the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
