{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Basic Mathematics Behind the Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "The **neural network (NN)** is a learner made of several **neuron** idealy which is similar to **boosting algorithem**, see [Machine Learning in Python/Chapter 7/Example 3](../../Machine_Learning_in_Python_SR/Chapter_07/example_03_adaBoost.ipynb), using many weaker learner to ensemble a strong learner. Each neuron or weaker learner is expected to be with responsibility a simple and particular model. The basic structure of the nueral network contains 3 layers : **input layer**, **hidden layers** and **output layer**, as in following figure: \n",
    "\n",
    "![nn](../doc/nn.png)\n",
    "(Credited from [\"Using neural nets to recognize handwritten digits\"](http://neuralnetworksanddeeplearning.com/chap1.html))\n",
    "\n",
    "The input layer is the first layer of neural network to encode the coming data to the hidden layers, and the number of the neurons, $n_{i}$, usually is the same with the dimension of the features; the hidden layers can contain the several layers, $l_{h}$, and the different sizes of neurons; And the output layer connects with the last layer of the hidden layers to decode thier results to final prediction, and the number of the neuron, $n_o$, is usually as same as the target class, of course it can be reduced or transfer to different representation. It is sometimes simply called \"**$n_il_hn_o$-NN**\". The neuron is actually made by the ***Character function*** and ***Active function*** which hold the data in mathematical way. The *Character function* is usually defined as \n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}\\cdot\\mathbf{X}+b\\ ,\n",
    "$$\n",
    "\n",
    "which is as the ***perceptron***, details see [Machine Learning in Python/Chapter 2](https://github.com/juifa-tsai/workbook_MachineLearning/blob/master/Machine_Learning_in_Python_SR/Chapter_02/README.md), where $\\mathbf{X}$ is the input data with the vector form which recores the features; and $\\mathbf{w}$ and $b$ are the weight vector correspoding to features and bias, respectively. Both variables are going to be learned during training. Then, the *Active function* is the fuction transfering the value of character fucntion to the \"proper\" output, e.g. the ***linear function*** or ***sigmoid function*** as \n",
    "\n",
    "$$\n",
    "a(z)=z\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "a(z)=\\frac{1}{1+\\exp{(-z)}}\\ , \n",
    "$$\n",
    "\n",
    "respectively. Of course there are several variant active functions can be used, but they are dependent on the property of the popurse and the chosen cost function. However, the neural netwrok can be either ***\"broader\"*** or ***\"deeper\"***, i.e. the number of neurons or layers can can be as many as possible, respectively. From the results of the expriences, the deeper neural network has better performance than broader one. The deeper type of the neural network is also called **Deeplearning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variables definition\n",
    "\n",
    "During the model learning (training), we start from the input layer. The neurons of the input layer will encode the input data and output the results by *character function* and *active function* to next hidden layer. Each neuron of the first hidden layer will receive the input from all the output of the input layer and return the new output for the next hidden layer again, i.e. the neuron caculates the output with the *character function* and *active function*. The next, even the next after the next, all the hidden layers will do the same procedure as the first layer. In the end, the output of the last hidden layer will pass through the output layer and decode to the prediction which we are interested. Since the learning procedure is from the layer to next layer, it is also called ***feedforward neural network***.   \n",
    "\n",
    "However, you can imagine that there must be many weights and biases during learning (training). To avoid the confusion, we have to give them the proper notations. Since all the variables are connected between two layers, we denote its layer is $l$, its one of neuron is $k$, and the input from one of the neurons from previous layer, $l-1$, is denoted $j$. Thus, the weight is redefined as $w_{jk}^l$, the example is illustrated in following figure\n",
    "\n",
    "![w](../doc/w.png) \n",
    "(Credited from [\"How the backpropagation algorithm works\"](http://neuralnetworksanddeeplearning.com/chap2.html))\n",
    "\n",
    "The bias and the value of *active function* are redifined as $b_{k}^l$ and $a_{k}^l$, the example is illustrated in following figure\n",
    "\n",
    "![b](../doc/ba.png)\n",
    "(Credited from [\"How the backpropagation algorithm works\"](http://neuralnetworksanddeeplearning.com/chap2.html))\n",
    "\n",
    "where $a_k^l$ is equal to $a(z_k^l)$ which the *character function* is redifined as\n",
    "\n",
    "$$\n",
    "z_k^l = \\sum_{j}w_{jk}^la_k^{l-1}+b_k^l\\ ,\n",
    "$$\n",
    "\n",
    "where $a_k^{l-1}$ is the output of *avctive function* from previous layer $l-1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation\n",
    "\n",
    "There are several alternative algorithms made with the different cost functions, active functions, regularization method, iteration method etc.. However, the main sprit of machine learning is not changed, which looks for the lower cost (error), i.e. the global minimum. But to caculate the cost and update the weights and biases for all neurons sounds crazy and impossible, since the dimensions of the variables are too huge. Fortunately, there is a simple and direct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
