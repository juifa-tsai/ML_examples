{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Basic Mathematics Behind the Neural Network\n",
    "---\n",
    "The **neural network (NN)** is a learner made of several **neuron** idealy which is similar to **boosting algorithem**, see [Machine Learning in Python/Chapter 7/Example 3](../../Machine_Learning_in_Python_SR/Chapter_07/example_03_adaBoost.ipynb), using many weaker learner to ensemble a strong learner. Each neuron or weaker learner is expected to be with responsibility a simple and particular model. The basic nueral network contains the 3 layers : **input layer**, **hidden layers** and **output layer**. The input layer is the first layer of neural network to encode the coming data to the hidden layers, and the number of the neurons, $n_{i}$, usually is the same with the dimension of the features; the hidden layers can contain the several layers, $l_{h}$, and the different sizes of neurons; And the output layer connects with the last layer of the hidden layers to decode thier results to final prediction, and the number of the neuron, $n_o$, is usually as same as the target class, of course it can be reduced or transfer to different representation. It is sometimes simply called \"**$n_il_hn_o$-NN**\". The neuron is actually made by the ***Character function*** and ***Active function*** which hold the data in mathematical way. The *Character function* is usually defined as \n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}\\mathbf{X}+b\\ ,\n",
    "$$\n",
    "\n",
    "which is as the *perceptron*, detials see [Machine Learning in Python/Chapter 2](../../Machine_Learning_in_Python_SR/Chapter_02), where $\\mathbf{X}$ is the input data with the vector form which recores the features; and $\\mathbf{w}$ and $b$ are the weight and bias that are going to be learned, respectively.\n",
    "The *Active function* is the fuction transfering the value of character fucntion to the \"proper\" output, e.g. the ***linear function*** or ***sigmoid function*** as \n",
    "\n",
    "$$\n",
    "a(z) = z\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "a(z)=\\frac{1}{1+\\exp{(-z)}}\\ , \n",
    "$$\n",
    "\n",
    "respectively. Of course there are several variant active functions can be used, but they are dependent on the property of the popurse and the chosen cost function. However, the neural netwrok can be either ***\"broader\"*** or ***\"deeper\"***, i.e. the number of neurons or layers can can be as many as possible, respectively. From the results of the expriences, the deeper neural network has better performance than broader one. The deeper type of the neural network is also called **Deeplearning**. \n",
    "\n",
    "There are several alternative algorithms made with the different cost functions, active functions, regularization method, iteration method etc.. However, the main sprit of machine learning is not changed, which looks for the lower cost (error) model. But to caculate the cost and update the weights and bias for each neuron sounds crazy and not doable, since the dimensions of tuning variables are too huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
