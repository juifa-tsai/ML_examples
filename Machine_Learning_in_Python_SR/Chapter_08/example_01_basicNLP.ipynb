{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Basic Techniques of Nature Language Processing\n",
    "---\n",
    "This is the example showing the basic techniques of **Nature language processing (NLP)**, since the this Chapter is focusing on the sentiment analysis which relates to language, i.e. the reviews of movie. In the preprocessing the data, we need following basic techniques.\n",
    "- **Bag-of-word**\n",
    "- **Term frequency-inverse document frequency, tf-idf**\n",
    "- **Porter stemmer algorithm**\n",
    "- **stop-word removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag-of-word\n",
    "It is also called **$n$-gram model**, $n$ is the number of words for a bag. The method is to count the used bag of words in sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1g = CountVectorizer()\n",
    "count_2g = CountVectorizer(ngram_range=(2,2))\n",
    "bag_1g = count_1g.fit_transform(docs)\n",
    "bag_2g = count_2g.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  {u'and': 0, u'weather': 6, u'sweet': 4, u'sun': 3, u'is': 1, u'the': 5, u'shining': 2}\n"
     ]
    }
   ],
   "source": [
    "print '1-gram: ', count_1g.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram:  {u'the sun': 5, u'shining and': 3, u'the weather': 6, u'sun is': 4, u'and the': 0, u'weather is': 7, u'is shining': 1, u'is sweet': 2}\n"
     ]
    }
   ],
   "source": [
    "print '2-gram: ', count_2g.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Term frequency-inverse document frequency, tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print tfidf.fit_transform(count_1g.fit_transform(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Porter stemmer algorithm\n",
    "The algorithm is plitting the words to the word stemming, i.e. runs -> run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer #pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runners like running and thus they run\n"
     ]
    }
   ],
   "source": [
    "text='runners like running and thus they run'\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runners -> runner\n",
      "like -> like\n",
      "running -> run\n",
      "and -> and\n",
      "thus -> thu\n",
      "they -> they\n",
      "run -> run\n",
      "[u'runner', 'like', u'run', 'and', u'thu', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "text_stem = []\n",
    "for word in text.split():\n",
    "    stem = porter.stem(word)\n",
    "    text_stem.append(stem)\n",
    "    print '%s -> %s'%( word, stem )\n",
    "print text_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop-wrod removal\n",
    "Remove the words without helpful meaning, e.g. is, the has etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Alpha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'runner', 'like', u'run', u'thu', 'run']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "text_stem_rm = []\n",
    "for w in text_stem:\n",
    "    if w not in stop:\n",
    "        text_stem_rm.append(w)\n",
    "print text_stem_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
