{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2. Sentiment training with IMBd data, Grid Searching\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package and function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # remove html flag, e.g. <br />\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())+' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "# test: \n",
    "preprocessor('</a>This :) is :( a test :-)!;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'runner', 'like', u'run', 'and', u'thu', 'they', 'run']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "# test:\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import IMBd movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Due to the huge size, keeps splitted train and test\n",
    "df_train = pd.read_csv('../data/imbd.csv.train')\n",
    "df_test  = pd.read_csv('../data/imbd.csv.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Yep, this has got to be one of the lamest movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>Gilmore Girls is one of the funniest, most cle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>Good luck finding this film to even watch - it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>Having watched 10 minutes of this movie I was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I really hate most end of the world movies. Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  Yep, this has got to be one of the lamest movi...          0\n",
       "19602  Gilmore Girls is one of the funniest, most cle...          1\n",
       "45519  Good luck finding this film to even watch - it...          1\n",
       "25747  Having watched 10 minutes of this movie I was ...          0\n",
       "42642  I really hate most end of the world movies. Th...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "df = df_test.append(df_train, ignore_index=True)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>yep this has got to be one of the lamest movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>gilmore girls is one of the funniest most clev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>good luck finding this film to even watch it s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>having watched 10 minutes of this movie i was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>i really hate most end of the world movies the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  yep this has got to be one of the lamest movie...          0\n",
       "19602  gilmore girls is one of the funniest most clev...          1\n",
       "45519  good luck finding this film to even watch it s...          1\n",
       "25747  having watched 10 minutes of this movie i was ...          0\n",
       "42642  i really hate most end of the world movies the...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test  = df.loc[25000:, 'review'].values\n",
    "y_test  = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training with grid-search of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection         import GridSearchCV\n",
    "from sklearn.model_selection         import cross_val_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Alpha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lr_tfidf = Pipeline([('vect', TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)),\n",
    "                     ('clf',  LogisticRegression(random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [ # defualt: use_idf=True, smooth_idf=True, norm=12\n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "               'vect__tokenizer'  : [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty'     : ['l1','l2'],\n",
    "               'clf__C'           : [1., 10., 100.]},\n",
    "               # defualt: use_idf=False, smooth_idf=False, norm=None\n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words' : [stop, None],\n",
    "               'vect__tokenizer'  : [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf'    : [False],\n",
    "               'vect__smooth_idf' : [False],\n",
    "               'vect__norm'       : [None],\n",
    "               'clf__penalty'     : ['l1','l2'],\n",
    "               'clf__C'           : [1., 10., 100.]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "# gs_lr_tfidf.fit(X_train, y_train)\n",
    "# print 'Best parameter set: %s' % gs_lr_tfidf.best_param_\n",
    "# print 'CV Accuracy: %.3f' %      gs_lr_tfidf.best_score_\n",
    "# print 'Test Accuracy: %.3f'%     gs_lr_tfidf.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "## But it take too long about > 40mins....\n",
    "## The best parameter set : \n",
    "## {'clf__C':10.0, 'vect__stop_words':None, 'clf__penalty':'l2', 'vect__tokenizer': <function tokenizer>}, 'vect__ngram_range': (1,1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Training with grid-search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'clf__C':10.0, \n",
    "          'clf__penalty':'l2', \n",
    "          'vect__stop_words':None, \n",
    "          'vect__tokenizer':tokenizer, \n",
    "          'vect__ngram_range': (1,1)}\n",
    "lr_tfidf.set_params(**params) # lr_tfidf.get_params()\n",
    "lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.892\n",
      "Test Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(estimator=lr_tfidf, X=X_train, y=y_train, cv=10, n_jobs=1)\n",
    "print 'CV Accuracy: %.3f' %  np.mean(scores)\n",
    "print 'Test Accuracy: %.3f'% lr_tfidf.score(X_test,  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conclusion\n",
    "Here I give the example to train the IMBd movie review according to the basic NLP techniques as in [Example 1](example_01_basicNLP.ipynb). Before the training, the grid-search is used for parameter optimization. However, the searching algorithm is too slow and inefficient, although it gives the good performace in the prediction from data. Thus, in next example, [Example 3](example_03_outofcore), I am going to introduce an alternative searching method for optimization, so call \"***out-of-core learning***\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
